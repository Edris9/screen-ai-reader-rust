# ğŸ”® Screen AI Reader

En snabb Rust-app fÃ¶r att fÃ¥nga delar av skÃ¤rmen och analysera med AI.

## Funktioner

- ğŸ“¸ **SkÃ¤rmfÃ¥ngst** - Dra en rektangel fÃ¶r att vÃ¤lja omrÃ¥de
- ğŸ–¥ï¸ **Lokal modell** - StÃ¶d fÃ¶r Ollama (LLaVA, Llama-vision, etc.)
- â˜ï¸ **Online modeller** - OpenAI GPT-4o och Claude stÃ¶d
- ğŸ“œ **Historik** - Spara tidigare analyser
- âš¡ **Streaming** - Se svaret medan det genereras
- ğŸ¨ **Lila tema** - Snygg modern design

## Installation

### FÃ¶rutsÃ¤ttningar

1. **Rust** - Installera frÃ¥n https://rustup.rs/
2. **Lokal modell** (valfritt) - Installera Ollama:
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ollama pull llava
   ```

### Bygg och kÃ¶r

```bash
# Klona/kopiera projektet
cd screen-ai-reader

# Bygg (release fÃ¶r snabbhet)
cargo build --release

# KÃ¶r
cargo run --release
```

## AnvÃ¤ndning

1. **Starta appen** - Ett lila fÃ¶nster Ã¶ppnas
2. **Klicka "ğŸ“¸ Ny SkÃ¤rmdump"** - SkÃ¤rmen mÃ¶rknar
3. **Dra en rektangel** - Markera omrÃ¥det du vill analysera
4. **Skriv prompt** (valfritt) - BerÃ¤tta vad AI:n ska gÃ¶ra
5. **Klicka "ğŸš€ Analysera"** - VÃ¤nta pÃ¥ svaret

### Tangentbord

- `ESC` - Avbryt skÃ¤rmfÃ¥ngst

## Konfiguration

Klicka pÃ¥ âš™ï¸ fÃ¶r att:
- StÃ¤lla in Ollama endpoint och modell
- LÃ¤gga till OpenAI API-nyckel
- LÃ¤gga till Claude API-nyckel
- Ã„ndra standard-prompt

Config sparas i:
- **Linux/Mac**: `~/.config/screen-ai-reader/config.json`
- **Windows**: `%APPDATA%\screen-ai-reader\config.json`

## Modeller

### Lokal (Ollama)
```bash
# Vision-modeller som fungerar:
ollama pull llava          # BÃ¤st balans
ollama pull llava:34b      # Mer kapabel
ollama pull bakllava       # Alternativ
```

### Online
- **OpenAI**: `gpt-4o` (rekommenderad), `gpt-4-vision-preview`
- **Claude**: `claude-sonnet-4-20250514` (snabb), `claude-opus-4-20250514` (smartast)

## Beroenden

- `eframe/egui` - GUI
- `screenshots` - SkÃ¤rmfÃ¥ngst
- `reqwest` - HTTP-requests
- `tokio` - Async runtime
- `serde` - Serialisering

## Tips fÃ¶r snabbhet

1. **AnvÃ¤nd release-build**: `cargo run --release`
2. **HÃ¥ll Ollama igÃ¥ng**: FÃ¶rsta requesten laddar modellen
3. **Mindre modeller Ã¤r snabbare**: `llava` istÃ¤llet fÃ¶r `llava:34b`
4. **Streaming**: Svaret bÃ¶rjar visas direkt

## Licens

MIT
